{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2dcfc-01bf-4be5-a6b1-6da084d6741a",
   "metadata": {},
   "source": [
    "# 1.什么是Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767781e-6a09-4f82-a8cb-ad90c3c0f50c",
   "metadata": {},
   "source": [
    "Word2Vec是一种用于将词汇转换为向量表示的技术，它是自然语言处理中的一种重要工具。它的核心思想是将词汇映射到一个高维空间中的向量，使得具有相似含义的词汇在这个空间中的向量表示也相互靠近。这种表示能够捕捉到词汇之间的语义关系，例如近义词在向量空间中距离较近，而语义上不相关的词汇则距离较远。\n",
    "\n",
    "Word2Vec模型主要有两种训练方法：Skip-gram和CBOW（Continuous Bag of Words）。Skip-gram模型通过给定中心词来预测上下文词汇，而CBOW则通过给定上下文词汇来预测中心词。这两种方法都能够学习到词汇的分布式表示，这些表示可以用作后续自然语言处理任务（如情感分析、命名实体识别等）的输入特征。\n",
    "\n",
    "总之，Word2Vec通过将词汇转换为向量，有效地捕捉了词汇之间的语义信息，为自然语言处理提供了强大的工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10914c83-9764-45c4-a830-77dd1fb994fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **人话：**把文字转成向量，放到一个空间里，把近义词 尽量 放一块，把反义词/不相关的词 尽量 放远一些 ，有点像语义分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b120933-d455-4ca4-996a-94f1a9cd827e",
   "metadata": {},
   "source": [
    "# 2.怎么实现Word2Vec，有哪些步骤，这些步骤作用是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e54ae3-3c7d-4b01-9a44-7242c23b1f33",
   "metadata": {},
   "source": [
    "## 步骤1.数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ea512-1979-4827-81da-48e4198d71c8",
   "metadata": {},
   "source": [
    "作用：将原始文本数据转化为适合模型处理的格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f2f6f-d730-4572-9517-86c246b743d7",
   "metadata": {},
   "source": [
    "## 步骤2. 构建词汇表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e2906-6427-4ef9-80ef-6a87e88129ec",
   "metadata": {},
   "source": [
    "作用：创建一个将词汇映射到唯一索引的数据结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30657d-aec4-454b-8070-5065c7fe2701",
   "metadata": {},
   "source": [
    "## 步骤3. 生成训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba551d1-b8ed-46d0-8ca8-385ddec4b563",
   "metadata": {},
   "source": [
    "作用：根据词汇表和上下文窗口创建训练样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3781a6-67db-4a48-8288-00924eac1057",
   "metadata": {},
   "source": [
    "## 步骤4. 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19569fdd-74a5-4a02-83f4-ea869474b977",
   "metadata": {},
   "source": [
    "作用：建立神经网络模型以进行词向量学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d7459-936e-4a41-a0b3-1cedb10a1d0b",
   "metadata": {},
   "source": [
    "## 步骤5. 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c0d26-b635-4f34-a8e6-fbc2073f35a4",
   "metadata": {},
   "source": [
    "作用：优化模型参数以最小化预测误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd23c47-ddfe-41a2-b2bd-01f65a7d4e69",
   "metadata": {},
   "source": [
    "## 步骤6. 评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaaa9ba-1cad-47cf-a296-8949e5ad4f30",
   "metadata": {},
   "source": [
    "作用：检查模型的性能，并提取训练好的词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733cd928-d87c-4502-b7a6-a66e87519a21",
   "metadata": {},
   "source": [
    "## 步骤7. 应用模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa5a3c-b28b-4ab0-8437-a0decb9e6e1d",
   "metadata": {},
   "source": [
    "作用：将训练好的词向量应用于实际的自然语言处理任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4518e-e848-4d6c-bca9-64df8deac026",
   "metadata": {},
   "source": [
    "# 3.代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abee82-f985-46cf-a86c-62d3712d00a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.1 数据预处理\n",
    "    1.去除文本中的标点符号和特殊字符，保留中文、英文和空格。\n",
    "    2.将文本中的中文和英文分开处理，并将它们合并成一个单词列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63562f67-c1d6-4096-a0c8-183fc5029ce3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.1.1 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ce847-8d6a-4ecb-9920-c8674a20c966",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **1.去除文本中的标点符号和特殊字符，保留中文、英文和空格。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd9d1a7-4af7-45ce-b9f0-c844141dc09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 去除所有标点符号和特殊字符（保留中英文和空格）\n",
    "    text = re.sub(r'[^\\w\\s\\u4e00-\\u9fff]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19413c4-8d07-454a-a614-9e48c7bcc52a",
   "metadata": {},
   "source": [
    "##### **示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5305c7e2-c854-4105-93aa-551bf7d36862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理后的文本: 我喜欢吃苹果 and I like bananas too\n"
     ]
    }
   ],
   "source": [
    "# 示例文本\n",
    "text = \"我喜欢吃苹果, and I like bananas too!\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "#去除了符号\n",
    "print(\"预处理后的文本:\", preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb65f9b3-4cd7-4471-bb8e-485bb79447da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **2.将文本中的中文和英文分开处理，并将它们合并成一个单词列表。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2205f360-a3d7-4963-95a4-3ae6990e8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # 提取中文字符\n",
    "    chinese_text = re.findall(r'[\\u4e00-\\u9fff]+', text)\n",
    "    \n",
    "    # 中文字符分词（每个字符作为一个词）\n",
    "    chinese_tokens = []\n",
    "    for chunk in chinese_text:\n",
    "        for char in chunk:\n",
    "            chinese_tokens.append(char)\n",
    "\n",
    "    # 提取英文单词（包括连续的英文单词）\n",
    "    english_tokens = re.findall(r'[a-zA-Z]+', text)\n",
    "    \n",
    "    # 合并所有分词结果\n",
    "    all_tokens = chinese_tokens + english_tokens\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b797dd4-d239-4023-8f0b-e103ef4e3daa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73198089-c50a-4fff-aec1-4da71aa23612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词后的文本数据: ['我', '喜', '欢', '吃', '苹', '果', 'and', 'I', 'like', 'bananas', 'too']\n"
     ]
    }
   ],
   "source": [
    "# 示例文本\n",
    "text = \"我喜欢吃苹果，and I like bananas too!\"\n",
    "tokens = tokenize_text(text)\n",
    "print(\"分词后的文本数据:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eff89e5-ed78-439f-b7f2-2e50584ce52d",
   "metadata": {},
   "source": [
    "#### **2.将文本中的中文和英文分开处理，并将它们合并成一个单词列表。（jieba版本）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ad709f60-777b-4bac-a2db-79cf39c19aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "\n",
    "def tokenize_text_jieba(text):\n",
    "    \"\"\"\n",
    "    使用 jieba 对中文进行分词，并对英文按空格分词。\n",
    "    \"\"\"\n",
    "    # 提取中文字符\n",
    "    chinese_text = re.findall(r'[\\u4e00-\\u9fff]+', text)\n",
    "    \n",
    "    # 中文字符分词\n",
    "    chinese_tokens = []\n",
    "    for chunk in chinese_text:\n",
    "        chinese_tokens.extend(jieba.cut(chunk))\n",
    "    \n",
    "    \n",
    "    # 提取英文单词（包括连续的英文单词）\n",
    "    english_tokens = re.findall(r'[a-zA-Z]+', text)\n",
    "    print(chinese_tokens)\n",
    "    print(english_tokens)\n",
    "    \n",
    "    # 合并所有分词结果\n",
    "    all_tokens = chinese_tokens + english_tokens\n",
    "    return all_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac5fe9-2f8b-4577-b7e4-e3c743da3fcd",
   "metadata": {},
   "source": [
    "##### **示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "10f4371b-9469-4c08-aa14-765a880196da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '喜欢', '吃', '苹果']\n",
      "['and', 'I', 'like', 'bananas', 'too']\n",
      "分词后的文本数据: ['我', '喜欢', '吃', '苹果', 'and', 'I', 'like', 'bananas', 'too']\n"
     ]
    }
   ],
   "source": [
    "# 示例文本\n",
    "text = \"我喜欢吃苹果 and I like bananas too!\"\n",
    "tokens = tokenize_text_jieba(text)\n",
    "print(\"分词后的文本数据:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db354a41-54fb-4f1d-9cc5-2e4bf517ec0d",
   "metadata": {},
   "source": [
    "## 3.2 构建词汇表\n",
    "    1.统计每个词在所有文档中出现的频次\n",
    "    2.根据词频统计构建词汇表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e20c1-d666-4e1b-9c5a-b63f477fc9fc",
   "metadata": {},
   "source": [
    "#### **1.统计每个词在所有文档中出现的频次。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1099f7a1-9320-4724-b6f9-df23688d778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_word_frequencies(documents):\n",
    "    all_words = []\n",
    "    for doc in documents:\n",
    "        for word in doc:\n",
    "            all_words.append(word)\n",
    "    #把所有词都放进数组后，使用Count方法进行统计\n",
    "    word_counts = Counter(all_words)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee39d9-007f-4f8f-9dec-c0776a792c7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "637d4858-839a-4643-9971-0a737c20f296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词频统计: Counter({'我': 3, '喜欢': 2, '吃': 2, '苹果': 2, '香蕉': 2, '和': 1, '都是': 1, '水果': 1, '汽车': 1, '很': 1, '方便': 1, '有': 1, '一只': 1, '狗': 1})\n"
     ]
    }
   ],
   "source": [
    "# 示例文档\n",
    "documents = [\n",
    "    ['我', '喜欢', '吃', '苹果'],\n",
    "    ['我', '喜欢', '吃', '香蕉'],\n",
    "    ['苹果', '和', '香蕉', '都是', '水果'],\n",
    "    ['汽车', '很', '方便'],\n",
    "    ['我', '有', '一只', '狗']\n",
    "]\n",
    "\n",
    "# 计算词频\n",
    "word_counts = count_word_frequencies(documents)\n",
    "print(\"词频统计:\", word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d06cff3-454e-4202-9aae-21f787433189",
   "metadata": {},
   "source": [
    "#### **2. 根据词频统计构建词汇表。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6200df5a-25ed-46cd-ac93-d22a6e147f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#词出现的越频繁，词在词汇表越前面\n",
    "def build_vocab(word_counts):\n",
    "    vocab = {}\n",
    "    print(word_counts.items())\n",
    "    for idx, (word, _) in enumerate(word_counts.items()):\n",
    "        #vocab[我]=当前序号\n",
    "        vocab[word] = idx\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a94b01-c264-48bd-a241-2463d9d7299a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "860b81df-826a-4193-ac56-e7d95d8aa311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词频统计: Counter({'我': 3, '喜欢': 2, '吃': 2, '苹果': 2, '香蕉': 2, '和': 1, '都是': 1, '水果': 1, '汽车': 1, '很': 1, '方便': 1, '有': 1, '一只': 1, '狗': 1})\n",
      "dict_items([('我', 3), ('喜欢', 2), ('吃', 2), ('苹果', 2), ('香蕉', 2), ('和', 1), ('都是', 1), ('水果', 1), ('汽车', 1), ('很', 1), ('方便', 1), ('有', 1), ('一只', 1), ('狗', 1)])\n",
      "词汇表: {'我': 0, '喜欢': 1, '吃': 2, '苹果': 3, '香蕉': 4, '和': 5, '都是': 6, '水果': 7, '汽车': 8, '很': 9, '方便': 10, '有': 11, '一只': 12, '狗': 13}\n"
     ]
    }
   ],
   "source": [
    "# 示例文档\n",
    "documents = [\n",
    "    ['我', '喜欢', '吃', '苹果'],\n",
    "    ['我', '喜欢', '吃', '香蕉'],\n",
    "    ['苹果', '和', '香蕉', '都是', '水果'],\n",
    "    ['汽车', '很', '方便'],\n",
    "    ['我', '有', '一只', '狗']\n",
    "]\n",
    "\n",
    "# 计算词频\n",
    "word_counts = count_word_frequencies(documents)\n",
    "print(\"词频统计:\", word_counts)\n",
    "\n",
    "# 构建词汇表\n",
    "vocab = build_vocab(word_counts)\n",
    "print(\"词汇表:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5129131b-02d1-4b4c-b503-b185347c7486",
   "metadata": {},
   "source": [
    "## 3.3 生成训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc5861-af7d-43cb-95d7-4be086ab7903",
   "metadata": {},
   "source": [
    "#### **定义 Word2VecDataset 类。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cb25a2ba-d32f-4e20-9e24-70b2c95f8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, documents, vocab, window_size=2):\n",
    "        \"\"\"\n",
    "        初始化数据集，生成上下文-中心词对。\n",
    "\n",
    "        参数:\n",
    "        - documents: 一个包含分词后的文档列表，每个文档是一个词的列表。\n",
    "        - vocab: 一个词汇表，将每个词映射到唯一的索引。\n",
    "        - window_size: 上下文窗口的大小，默认为 2。\n",
    "        \"\"\"\n",
    "        self.window_size = window_size  # 上下文窗口的大小\n",
    "        self.vocab = vocab  # 词汇表\n",
    "        self.pairs = self._create_pairs(documents)  # 生成上下文-中心词对\n",
    "    \n",
    "    def _create_pairs(self, documents):\n",
    "        \"\"\"\n",
    "        创建上下文-中心词对。\n",
    "\n",
    "        参数:\n",
    "        - documents: 一个包含分词后的文档列表，每个文档是一个词的列表。\n",
    "\n",
    "        返回:\n",
    "        - pairs: 一个包含上下文-中心词对的列表。\n",
    "        \"\"\"\n",
    "        pairs = []  # 用于存储上下文-中心词对的列表\n",
    "        for doc in documents:\n",
    "            for idx, word in enumerate(doc):\n",
    "                word_idx = self.vocab[word]  # 获取中心词的索引\n",
    "                # 定义上下文窗口的起始和结束位置\n",
    "                #开始的索引 从0与 当前索引-窗口大小值 中选，也就是说如果减出来的值小于0 就为0\n",
    "                start = max(0, idx - self.window_size)\n",
    "                #结束的索引 从 数据集的长度和 当前索引窗口+窗口大小+1 决定\n",
    "                ## +1 是因为 range 函数的 end 是不包括的\n",
    "                end = min(len(doc), idx + self.window_size + 1)\n",
    "                # 遍历上下文窗口中的所有词\n",
    "                for context_idx in range(start, end):\n",
    "                    if context_idx != idx:  # 跳过中心词\n",
    "                        #当前中心词范围的值\n",
    "                        context_word = doc[context_idx]\n",
    "                        #当前中心词范围值的索引\n",
    "                        context_word_idx = self.vocab[context_word]  # 获取上下文词的索引\n",
    "                        pairs.append((word_idx, context_word_idx))  # 添加上下文-中心词对到列表\n",
    "                        # 上下文词索引和中心词对的生成：\n",
    "                        \n",
    "                        # context_idx = 0，上下文词为 '我'，索引为 0，对 (2, 0)。\n",
    "                        # context_idx = 1，上下文词为 '喜欢'，索引为 1，对 (2, 1)。\n",
    "                        # context_idx = 3，上下文词为 '苹果'，索引为 3，对 (2, 3)。\n",
    "                        # 最终生成的 pairs 列表包含：[(2, 0), (2, 1), (2, 3)]。\n",
    "                        \n",
    "        return pairs\n",
    "\n",
    "    # pairs 列表生成的例子\n",
    "# pairs = [\n",
    "#     (0, 1), (1, 0), (1, 2), (2, 1), (2, 3), (3, 2),  # 文档 1: ['我', '喜欢', '吃', '苹果']\n",
    "#     (0, 1), (1, 0), (1, 2), (2, 1), (2, 4), (4, 2),  # 文档 2: ['我', '喜欢', '吃', '香蕉']\n",
    "#     (3, 5), (3, 4), (4, 3), (4, 5), (4, 6), (6, 4), (6, 7), (7, 6),  # 文档 3: ['苹果', '和', '香蕉', '都是', '水果']\n",
    "#     (8, 9), (9, 8), (9, 10), (10, 9),  # 文档 4: ['汽车', '很', '方便']\n",
    "#     (0, 11), (11, 0), (11, 12), (12, 11), (12, 13), (13, 12)  # 文档 5: ['我', '有', '一只', '狗']\n",
    "# ]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集中样本的总数。\n",
    "        \"\"\"\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        返回指定索引的样本。\n",
    "\n",
    "        参数:\n",
    "        - idx: 样本的索引。\n",
    "\n",
    "        返回:\n",
    "        - 一个包含中心词和上下文词索引的元组。\n",
    "        \"\"\"\n",
    "        center, context = self.pairs[idx]\n",
    "        #print(\"中心词索引:\", center)  # tensor(0)\n",
    "        #print(\"上下文词索引:\", context)  # tensor(1)\n",
    "        return torch.tensor(center, dtype=torch.long), torch.tensor(context, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9cfbc2-b5c0-4dd0-984c-6f8b262f69c2",
   "metadata": {},
   "source": [
    "##### **示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "84f0e2cb-92b0-4db3-ac69-b02eeb251aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中心词索引: tensor([5, 4])\n",
      "上下文词索引: tensor([4, 2])\n",
      "中心词索引: tensor([12,  3])\n",
      "上下文词索引: tensor([11,  1])\n",
      "中心词索引: tensor([5, 0])\n",
      "上下文词索引: tensor([3, 2])\n",
      "中心词索引: tensor([6, 9])\n",
      "上下文词索引: tensor([4, 8])\n",
      "中心词索引: tensor([11,  7])\n",
      "上下文词索引: tensor([12,  6])\n",
      "中心词索引: tensor([ 0, 13])\n",
      "上下文词索引: tensor([12, 12])\n",
      "中心词索引: tensor([ 2, 12])\n",
      "上下文词索引: tensor([4, 0])\n",
      "中心词索引: tensor([4, 1])\n",
      "上下文词索引: tensor([5, 2])\n",
      "中心词索引: tensor([8, 4])\n",
      "上下文词索引: tensor([10,  3])\n",
      "中心词索引: tensor([2, 0])\n",
      "上下文词索引: tensor([3, 1])\n",
      "中心词索引: tensor([ 2, 10])\n",
      "上下文词索引: tensor([1, 9])\n",
      "中心词索引: tensor([1, 3])\n",
      "上下文词索引: tensor([0, 5])\n",
      "中心词索引: tensor([9, 2])\n",
      "上下文词索引: tensor([10,  0])\n",
      "中心词索引: tensor([6, 4])\n",
      "上下文词索引: tensor([7, 1])\n",
      "中心词索引: tensor([7, 1])\n",
      "上下文词索引: tensor([4, 0])\n",
      "中心词索引: tensor([11,  1])\n",
      "上下文词索引: tensor([0, 2])\n",
      "中心词索引: tensor([4, 1])\n",
      "上下文词索引: tensor([7, 3])\n",
      "中心词索引: tensor([8, 2])\n",
      "上下文词索引: tensor([9, 1])\n",
      "中心词索引: tensor([4, 0])\n",
      "上下文词索引: tensor([6, 1])\n",
      "中心词索引: tensor([ 3, 10])\n",
      "上下文词索引: tensor([4, 8])\n",
      "中心词索引: tensor([13,  0])\n",
      "上下文词索引: tensor([11,  2])\n",
      "中心词索引: tensor([ 3, 12])\n",
      "上下文词索引: tensor([ 2, 13])\n",
      "中心词索引: tensor([5, 1])\n",
      "上下文词索引: tensor([6, 4])\n",
      "中心词索引: tensor([11,  0])\n",
      "上下文词索引: tensor([13, 11])\n",
      "中心词索引: tensor([6, 2])\n",
      "上下文词索引: tensor([5, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 创建数据集\n",
    "dataset = Word2VecDataset(documents, vocab)\n",
    "#因为被打乱了所以是乱序输出\n",
    "#batch_size=2 一次输出2个\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# 数据集划分为训练集和验证集\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 打印示例\n",
    "for center, context in dataloader:\n",
    "    print(\"中心词索引:\", center)\n",
    "    print(\"上下文词索引:\", context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f626d-dd7b-4cc2-bd8f-4bbc2d566f41",
   "metadata": {},
   "source": [
    "## 3.4 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9eff3d-1406-46db-888f-21b7e04c7069",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **基础模型：embedding。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a59a3ede-c166-444b-8644-b828263d337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyEmbedding:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        初始化词嵌入矩阵。\n",
    "\n",
    "        参数:\n",
    "        - vocab_size: 词汇表的大小\n",
    "        - embedding_dim: 嵌入向量的维度\n",
    "        \"\"\"\n",
    "        # 初始化嵌入矩阵为随机值\n",
    "        self.embeddings = torch.randn(vocab_size, embedding_dim, requires_grad=True)\n",
    "\n",
    "    def forward(self, indices):\n",
    "        \"\"\"\n",
    "        根据词索引获取嵌入向量。\n",
    "\n",
    "        参数:\n",
    "\n",
    "        indices：类似于学生的学号（或索引），你用它来查找具体的学生档案。\n",
    "        self.embeddings[indices]：根据学号（或索引），从档案表格中提取出相应的档案（嵌入向量）。\n",
    "        返回:\n",
    "        - 对应的嵌入向量（张量）\n",
    "        \"\"\"\n",
    "        return self.embeddings[indices]\n",
    "\n",
    "    def __getitem__(self, indices):\n",
    "        \"\"\"\n",
    "        支持索引操作以获取嵌入向量。\n",
    "\n",
    "        参数:\n",
    "        - indices: 词的索引（可以是一个列表或张量）\n",
    "\n",
    "        返回:\n",
    "        - 对应的嵌入向量（张量）\n",
    "        \"\"\"\n",
    "        return self.forward(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1c44ac49-fe90-4aad-b8e6-6a34c9d76cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词嵌入矩阵:\n",
      " tensor([[-1.3036,  0.2762,  1.2268,  1.4505,  0.0286],\n",
      "        [ 0.4403,  0.2579, -0.1095, -0.5406, -0.4847],\n",
      "        [ 2.7089, -0.1263, -0.6482,  1.1521, -1.1351],\n",
      "        [ 0.2351, -1.2752,  1.5673,  1.0044,  1.0660],\n",
      "        [-0.2077,  1.2500,  0.6497,  1.1076, -0.5300],\n",
      "        [ 1.9304, -0.7198, -1.7162,  0.5596,  1.7736],\n",
      "        [-0.6066, -0.2974,  0.0458, -1.6658,  1.6694],\n",
      "        [-0.7357, -0.1079, -0.1545, -0.4720,  0.7539],\n",
      "        [ 0.1120, -0.7730,  0.7116, -0.3670,  0.1294],\n",
      "        [ 1.8195,  0.0517,  2.6299,  0.1626,  1.7854]], requires_grad=True)\n",
      "获取的词嵌入:\n",
      " tensor([[-1.3036,  0.2762,  1.2268,  1.4505,  0.0286],\n",
      "        [ 0.4403,  0.2579, -0.1095, -0.5406, -0.4847],\n",
      "        [ 2.7089, -0.1263, -0.6482,  1.1521, -1.1351],\n",
      "        [ 0.2351, -1.2752,  1.5673,  1.0044,  1.0660]],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 示例：\n",
    "# 假设你有一个班级有 10 名学生，每个学生的档案包含 5 个特征（例如成绩、身高、体重等）。你用学生的学号来从档案表格中提取学生的档案信息。\n",
    "\n",
    "#\n",
    "# 在这个例子中，词嵌入就是班级的学生档案。\n",
    "# 你为每个学生创建一个唯一的档案（词向量），并能通过学号（索引）快速查找每个学生的档案。\n",
    "# MyEmbedding 类通过这种方式存储和检索每个词的嵌入向量。\n",
    "\n",
    "# 示例\n",
    "vocab_size = 10  # 词汇表大小\n",
    "embedding_dim = 5  # 嵌入向量的维度\n",
    "\n",
    "# 创建词嵌入对象\n",
    "embedding = MyEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "# 示例词索引\n",
    "indices = torch.tensor([0, 1, 2, 3])\n",
    "\n",
    "# 获取词嵌入\n",
    "embeds = embedding.forward(indices)\n",
    "\n",
    "print(\"词嵌入矩阵:\\n\", embedding.embeddings)\n",
    "print(\"获取的词嵌入:\\n\", embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764f0da-9ef6-4451-98f0-8a542c9f1939",
   "metadata": {},
   "source": [
    "#### **Word2Vec模型：Skip-Gram。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "14a5e5bf-7911-454b-971b-4e73db32f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        # 学生兴趣档案（输入词嵌入矩阵）\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 课程特征档案（输出词嵌入矩阵）\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 损失函数\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, center):\n",
    "        in_embeds = self.in_embeddings(center)\n",
    "        return in_embeds\n",
    "        \n",
    "    def loss(self, scores, target):\n",
    "        return self.loss_fn(scores, target)  # target 应该是 (batch_size,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3823c125-026d-446d-9099-13133e51a0da",
   "metadata": {},
   "source": [
    "## 3.5 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abefc2a-2a81-4086-a537-f2d19d3cd69f",
   "metadata": {},
   "source": [
    "#### **拆分后的代码**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7d9eed47-5634-46a9-8a55-460951f9f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 初始化模型\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100  # 嵌入维度\n",
    "model = SkipGram(vocab_size, embedding_dim)\n",
    "\n",
    "# 选择优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ecb4b3b5-4e5e-4fa0-8a8d-3e061d8a7576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.1250261294841766\n",
      "Epoch 2/5, Loss: 1.1370073390007018\n",
      "Epoch 3/5, Loss: 1.167758914232254\n",
      "Epoch 4/5, Loss: 1.126306984424591\n",
      "Epoch 5/5, Loss: 1.1364572167396545\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 训练超参数\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    #放训练集训练\n",
    "    for center, context in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        # 通过模型的前向传播方法，获取当前批次中每个中心词的嵌入向量\n",
    "        center_embeds = model(center)  # (batch_size, embedding_dim)\n",
    "\n",
    "        # 获取模型中所有上下文词的嵌入矩阵，形状为 (vocab_size, embedding_dim)\n",
    "        # 这里的上下文词嵌入矩阵是通过模型的输出嵌入层的权重获得的\n",
    "        context_embeds = model.out_embeddings.weight  # (vocab_size, embedding_dim)\n",
    "        \n",
    "        # 计算中心词嵌入与所有上下文词嵌入之间的点积\n",
    "        # center_embeds 形状为 (batch_size, embedding_dim)\n",
    "        # context_embeds.t() 形状为 (embedding_dim, vocab_size)，是上下文词嵌入矩阵的转置\n",
    "        # 结果 scores 形状为 (batch_size, vocab_size)，表示每个中心词对所有可能上下文词的得分\n",
    "        scores = torch.matmul(center_embeds, context_embeds.t())\n",
    "        \n",
    "        # CrossEntropyLoss 需要 target 是正确的类别索引\n",
    "        loss = model.loss(scores, context)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c585e07d-a48f-4f20-af5d-3023696747c2",
   "metadata": {},
   "source": [
    "#### **完整代码**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "b7bfa811-1a23-4051-8ffe-0ba5d0cc1ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 11.065189571380616\n",
      "Epoch 2/5, Loss: 9.265730533599854\n",
      "Epoch 3/5, Loss: 7.8241242408752445\n",
      "Epoch 4/5, Loss: 6.782663764953614\n",
      "Epoch 5/5, Loss: 5.937912817001343\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 初始化模型\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100  # 嵌入维度\n",
    "model = SkipGram(vocab_size, embedding_dim)\n",
    "\n",
    "# 选择优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练超参数\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for center, context in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        # 通过模型的前向传播方法，获取当前批次中每个中心词的嵌入向量\n",
    "        center_embeds = model(center)  # (batch_size, embedding_dim)\n",
    "\n",
    "        # 获取模型中所有上下文词的嵌入矩阵，形状为 (vocab_size, embedding_dim)\n",
    "        # 这里的上下文词嵌入矩阵是通过模型的输出嵌入层的权重获得的\n",
    "        context_embeds = model.out_embeddings.weight  # (vocab_size, embedding_dim)\n",
    "        \n",
    "        # 计算中心词嵌入与所有上下文词嵌入之间的点积\n",
    "        # center_embeds 形状为 (batch_size, embedding_dim)\n",
    "        # context_embeds.t() 形状为 (embedding_dim, vocab_size)，是上下文词嵌入矩阵的转置\n",
    "        # 结果 scores 形状为 (batch_size, vocab_size)，表示每个中心词对所有可能上下文词的得分\n",
    "        scores = torch.matmul(center_embeds, context_embeds.t())\n",
    "        \n",
    "        # CrossEntropyLoss 需要 target 是正确的类别索引\n",
    "        loss = model.loss(scores, context)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131fdb8-362b-4322-bf01-fd94d277b906",
   "metadata": {},
   "source": [
    "## 3.6 评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d47aa4-18cb-4e5c-9cf5-26ef1c41776b",
   "metadata": {},
   "source": [
    "#### **评估模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d20fbb6d-6999-4760-9274-287f65ad3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for center, context in dataloader:\n",
    "            center_embeds = model(center)\n",
    "            context_embeds = model.out_embeddings.weight\n",
    "            scores = torch.matmul(center_embeds, context_embeds.t())\n",
    "            loss = model.loss(scores, context)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "70d2cff6-7f7e-4de3-87e6-e426b7e60cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 7.422391021251679, Val Loss: 14.03045883178711\n"
     ]
    }
   ],
   "source": [
    "avg_loss = total_loss / len(train_dataloader)\n",
    "val_loss = evaluate(model, val_dataloader)  # 在验证集上评估损失\n",
    "print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}, Val Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29f719-753e-443b-9b40-3e5c0d345b37",
   "metadata": {},
   "source": [
    "#### **保存模型:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7b2b4b16-6b64-4edd-a943-8455077a7cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练和评估完成，模型已保存！\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'skipgram_model.pth')\n",
    "print(\"训练和评估完成，模型已保存！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a34f34-d081-432c-aa95-d7a73d216826",
   "metadata": {},
   "source": [
    "#### **可视化词嵌入(TODO)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af9cef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
